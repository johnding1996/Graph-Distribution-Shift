Dataset: ogb-molpcba
Algorithm: deepCORAL
Root dir: data
Split scheme: official
Dataset kwargs: {}
Download: False
Frac: 0.08
Version: None
Loader kwargs: {'num_workers': 4, 'pin_memory': True}
Train loader: group
Uniform over groups: True
Distinct groups: True
N groups per batch: 4
Batch size: 32
Eval loader: standard
Model: gin-virtual
Model kwargs: {'dropout': 0.5}
Transform: None
Target resolution: None
Resize scale: None
Max token length: None
Loss function: multitask_bce
Loss kwargs: {}
Groupby fields: ['scaffold']
Group dro step size: None
Coral penalty weight: 0.1
Irm lambda: 1.0
Irm penalty anneal iters: None
Algo log metric: multitask_binary_accuracy
Val metric: ap
Val metric decreasing: False
N epochs: 100
Optimizer: Adam
Lr: 0.001
Weight decay: 0.0
Max grad norm: None
Optimizer kwargs: {}
Scheduler: None
Scheduler kwargs: {}
Scheduler metric split: val
Scheduler metric name: None
Process outputs function: None
Evaluate all splits: True
Eval splits: []
Eval only: False
Eval epoch: None
Device: cuda:0
Seed: 0
Log dir: ./logs
Log every: 50
Save step: None
Save best: True
Save last: True
Save pred: True
No group logging: True
Use wandb: False
Progress bar: False
Resume: False

Train data...
    n = 28027
Validation data...
    n = 3503
Test data...
    n = 3503

Epoch [0]:

Train:
objective: 0.783
penalty: 1.051
loss_all: 0.678
acc_all: 0.723

objective: 0.817
penalty: 2.311
loss_all: 0.585
acc_all: 0.875

objective: 0.675
penalty: 2.926
loss_all: 0.382
acc_all: 0.947

objective: 0.284
penalty: 0.910
loss_all: 0.193
acc_all: 0.977

