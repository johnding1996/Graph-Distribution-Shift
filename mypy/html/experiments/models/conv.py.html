<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" type="text/css" href="../../../mypy-html.css">
</head>
<body>
<h2>experiments.models.conv</h2>
<table>
<caption>experiments/models/conv.py</caption>
<tbody><tr>
<td class="table-lines"><pre><span id="L1" class="lineno"><a class="lineno" href="#L1">1</a></span>
<span id="L2" class="lineno"><a class="lineno" href="#L2">2</a></span>
<span id="L3" class="lineno"><a class="lineno" href="#L3">3</a></span>
<span id="L4" class="lineno"><a class="lineno" href="#L4">4</a></span>
<span id="L5" class="lineno"><a class="lineno" href="#L5">5</a></span>
<span id="L6" class="lineno"><a class="lineno" href="#L6">6</a></span>
<span id="L7" class="lineno"><a class="lineno" href="#L7">7</a></span>
<span id="L8" class="lineno"><a class="lineno" href="#L8">8</a></span>
<span id="L9" class="lineno"><a class="lineno" href="#L9">9</a></span>
<span id="L10" class="lineno"><a class="lineno" href="#L10">10</a></span>
<span id="L11" class="lineno"><a class="lineno" href="#L11">11</a></span>
<span id="L12" class="lineno"><a class="lineno" href="#L12">12</a></span>
<span id="L13" class="lineno"><a class="lineno" href="#L13">13</a></span>
<span id="L14" class="lineno"><a class="lineno" href="#L14">14</a></span>
<span id="L15" class="lineno"><a class="lineno" href="#L15">15</a></span>
<span id="L16" class="lineno"><a class="lineno" href="#L16">16</a></span>
<span id="L17" class="lineno"><a class="lineno" href="#L17">17</a></span>
<span id="L18" class="lineno"><a class="lineno" href="#L18">18</a></span>
<span id="L19" class="lineno"><a class="lineno" href="#L19">19</a></span>
<span id="L20" class="lineno"><a class="lineno" href="#L20">20</a></span>
<span id="L21" class="lineno"><a class="lineno" href="#L21">21</a></span>
<span id="L22" class="lineno"><a class="lineno" href="#L22">22</a></span>
<span id="L23" class="lineno"><a class="lineno" href="#L23">23</a></span>
<span id="L24" class="lineno"><a class="lineno" href="#L24">24</a></span>
<span id="L25" class="lineno"><a class="lineno" href="#L25">25</a></span>
<span id="L26" class="lineno"><a class="lineno" href="#L26">26</a></span>
<span id="L27" class="lineno"><a class="lineno" href="#L27">27</a></span>
<span id="L28" class="lineno"><a class="lineno" href="#L28">28</a></span>
<span id="L29" class="lineno"><a class="lineno" href="#L29">29</a></span>
<span id="L30" class="lineno"><a class="lineno" href="#L30">30</a></span>
<span id="L31" class="lineno"><a class="lineno" href="#L31">31</a></span>
<span id="L32" class="lineno"><a class="lineno" href="#L32">32</a></span>
<span id="L33" class="lineno"><a class="lineno" href="#L33">33</a></span>
<span id="L34" class="lineno"><a class="lineno" href="#L34">34</a></span>
<span id="L35" class="lineno"><a class="lineno" href="#L35">35</a></span>
<span id="L36" class="lineno"><a class="lineno" href="#L36">36</a></span>
<span id="L37" class="lineno"><a class="lineno" href="#L37">37</a></span>
<span id="L38" class="lineno"><a class="lineno" href="#L38">38</a></span>
<span id="L39" class="lineno"><a class="lineno" href="#L39">39</a></span>
<span id="L40" class="lineno"><a class="lineno" href="#L40">40</a></span>
<span id="L41" class="lineno"><a class="lineno" href="#L41">41</a></span>
<span id="L42" class="lineno"><a class="lineno" href="#L42">42</a></span>
<span id="L43" class="lineno"><a class="lineno" href="#L43">43</a></span>
<span id="L44" class="lineno"><a class="lineno" href="#L44">44</a></span>
<span id="L45" class="lineno"><a class="lineno" href="#L45">45</a></span>
<span id="L46" class="lineno"><a class="lineno" href="#L46">46</a></span>
<span id="L47" class="lineno"><a class="lineno" href="#L47">47</a></span>
<span id="L48" class="lineno"><a class="lineno" href="#L48">48</a></span>
<span id="L49" class="lineno"><a class="lineno" href="#L49">49</a></span>
<span id="L50" class="lineno"><a class="lineno" href="#L50">50</a></span>
<span id="L51" class="lineno"><a class="lineno" href="#L51">51</a></span>
<span id="L52" class="lineno"><a class="lineno" href="#L52">52</a></span>
<span id="L53" class="lineno"><a class="lineno" href="#L53">53</a></span>
<span id="L54" class="lineno"><a class="lineno" href="#L54">54</a></span>
<span id="L55" class="lineno"><a class="lineno" href="#L55">55</a></span>
<span id="L56" class="lineno"><a class="lineno" href="#L56">56</a></span>
<span id="L57" class="lineno"><a class="lineno" href="#L57">57</a></span>
<span id="L58" class="lineno"><a class="lineno" href="#L58">58</a></span>
<span id="L59" class="lineno"><a class="lineno" href="#L59">59</a></span>
<span id="L60" class="lineno"><a class="lineno" href="#L60">60</a></span>
<span id="L61" class="lineno"><a class="lineno" href="#L61">61</a></span>
<span id="L62" class="lineno"><a class="lineno" href="#L62">62</a></span>
<span id="L63" class="lineno"><a class="lineno" href="#L63">63</a></span>
<span id="L64" class="lineno"><a class="lineno" href="#L64">64</a></span>
<span id="L65" class="lineno"><a class="lineno" href="#L65">65</a></span>
<span id="L66" class="lineno"><a class="lineno" href="#L66">66</a></span>
<span id="L67" class="lineno"><a class="lineno" href="#L67">67</a></span>
<span id="L68" class="lineno"><a class="lineno" href="#L68">68</a></span>
<span id="L69" class="lineno"><a class="lineno" href="#L69">69</a></span>
<span id="L70" class="lineno"><a class="lineno" href="#L70">70</a></span>
<span id="L71" class="lineno"><a class="lineno" href="#L71">71</a></span>
<span id="L72" class="lineno"><a class="lineno" href="#L72">72</a></span>
<span id="L73" class="lineno"><a class="lineno" href="#L73">73</a></span>
<span id="L74" class="lineno"><a class="lineno" href="#L74">74</a></span>
<span id="L75" class="lineno"><a class="lineno" href="#L75">75</a></span>
<span id="L76" class="lineno"><a class="lineno" href="#L76">76</a></span>
<span id="L77" class="lineno"><a class="lineno" href="#L77">77</a></span>
<span id="L78" class="lineno"><a class="lineno" href="#L78">78</a></span>
<span id="L79" class="lineno"><a class="lineno" href="#L79">79</a></span>
<span id="L80" class="lineno"><a class="lineno" href="#L80">80</a></span>
<span id="L81" class="lineno"><a class="lineno" href="#L81">81</a></span>
<span id="L82" class="lineno"><a class="lineno" href="#L82">82</a></span>
<span id="L83" class="lineno"><a class="lineno" href="#L83">83</a></span>
<span id="L84" class="lineno"><a class="lineno" href="#L84">84</a></span>
<span id="L85" class="lineno"><a class="lineno" href="#L85">85</a></span>
<span id="L86" class="lineno"><a class="lineno" href="#L86">86</a></span>
<span id="L87" class="lineno"><a class="lineno" href="#L87">87</a></span>
<span id="L88" class="lineno"><a class="lineno" href="#L88">88</a></span>
<span id="L89" class="lineno"><a class="lineno" href="#L89">89</a></span>
<span id="L90" class="lineno"><a class="lineno" href="#L90">90</a></span>
<span id="L91" class="lineno"><a class="lineno" href="#L91">91</a></span>
<span id="L92" class="lineno"><a class="lineno" href="#L92">92</a></span>
<span id="L93" class="lineno"><a class="lineno" href="#L93">93</a></span>
<span id="L94" class="lineno"><a class="lineno" href="#L94">94</a></span>
<span id="L95" class="lineno"><a class="lineno" href="#L95">95</a></span>
<span id="L96" class="lineno"><a class="lineno" href="#L96">96</a></span>
<span id="L97" class="lineno"><a class="lineno" href="#L97">97</a></span>
<span id="L98" class="lineno"><a class="lineno" href="#L98">98</a></span>
<span id="L99" class="lineno"><a class="lineno" href="#L99">99</a></span>
<span id="L100" class="lineno"><a class="lineno" href="#L100">100</a></span>
<span id="L101" class="lineno"><a class="lineno" href="#L101">101</a></span>
<span id="L102" class="lineno"><a class="lineno" href="#L102">102</a></span>
<span id="L103" class="lineno"><a class="lineno" href="#L103">103</a></span>
<span id="L104" class="lineno"><a class="lineno" href="#L104">104</a></span>
<span id="L105" class="lineno"><a class="lineno" href="#L105">105</a></span>
<span id="L106" class="lineno"><a class="lineno" href="#L106">106</a></span>
<span id="L107" class="lineno"><a class="lineno" href="#L107">107</a></span>
<span id="L108" class="lineno"><a class="lineno" href="#L108">108</a></span>
<span id="L109" class="lineno"><a class="lineno" href="#L109">109</a></span>
<span id="L110" class="lineno"><a class="lineno" href="#L110">110</a></span>
<span id="L111" class="lineno"><a class="lineno" href="#L111">111</a></span>
<span id="L112" class="lineno"><a class="lineno" href="#L112">112</a></span>
<span id="L113" class="lineno"><a class="lineno" href="#L113">113</a></span>
<span id="L114" class="lineno"><a class="lineno" href="#L114">114</a></span>
<span id="L115" class="lineno"><a class="lineno" href="#L115">115</a></span>
<span id="L116" class="lineno"><a class="lineno" href="#L116">116</a></span>
<span id="L117" class="lineno"><a class="lineno" href="#L117">117</a></span>
<span id="L118" class="lineno"><a class="lineno" href="#L118">118</a></span>
<span id="L119" class="lineno"><a class="lineno" href="#L119">119</a></span>
<span id="L120" class="lineno"><a class="lineno" href="#L120">120</a></span>
<span id="L121" class="lineno"><a class="lineno" href="#L121">121</a></span>
<span id="L122" class="lineno"><a class="lineno" href="#L122">122</a></span>
<span id="L123" class="lineno"><a class="lineno" href="#L123">123</a></span>
<span id="L124" class="lineno"><a class="lineno" href="#L124">124</a></span>
<span id="L125" class="lineno"><a class="lineno" href="#L125">125</a></span>
<span id="L126" class="lineno"><a class="lineno" href="#L126">126</a></span>
<span id="L127" class="lineno"><a class="lineno" href="#L127">127</a></span>
<span id="L128" class="lineno"><a class="lineno" href="#L128">128</a></span>
<span id="L129" class="lineno"><a class="lineno" href="#L129">129</a></span>
<span id="L130" class="lineno"><a class="lineno" href="#L130">130</a></span>
<span id="L131" class="lineno"><a class="lineno" href="#L131">131</a></span>
<span id="L132" class="lineno"><a class="lineno" href="#L132">132</a></span>
<span id="L133" class="lineno"><a class="lineno" href="#L133">133</a></span>
<span id="L134" class="lineno"><a class="lineno" href="#L134">134</a></span>
<span id="L135" class="lineno"><a class="lineno" href="#L135">135</a></span>
<span id="L136" class="lineno"><a class="lineno" href="#L136">136</a></span>
<span id="L137" class="lineno"><a class="lineno" href="#L137">137</a></span>
<span id="L138" class="lineno"><a class="lineno" href="#L138">138</a></span>
<span id="L139" class="lineno"><a class="lineno" href="#L139">139</a></span>
<span id="L140" class="lineno"><a class="lineno" href="#L140">140</a></span>
<span id="L141" class="lineno"><a class="lineno" href="#L141">141</a></span>
<span id="L142" class="lineno"><a class="lineno" href="#L142">142</a></span>
<span id="L143" class="lineno"><a class="lineno" href="#L143">143</a></span>
<span id="L144" class="lineno"><a class="lineno" href="#L144">144</a></span>
<span id="L145" class="lineno"><a class="lineno" href="#L145">145</a></span>
<span id="L146" class="lineno"><a class="lineno" href="#L146">146</a></span>
<span id="L147" class="lineno"><a class="lineno" href="#L147">147</a></span>
<span id="L148" class="lineno"><a class="lineno" href="#L148">148</a></span>
<span id="L149" class="lineno"><a class="lineno" href="#L149">149</a></span>
<span id="L150" class="lineno"><a class="lineno" href="#L150">150</a></span>
<span id="L151" class="lineno"><a class="lineno" href="#L151">151</a></span>
<span id="L152" class="lineno"><a class="lineno" href="#L152">152</a></span>
<span id="L153" class="lineno"><a class="lineno" href="#L153">153</a></span>
<span id="L154" class="lineno"><a class="lineno" href="#L154">154</a></span>
<span id="L155" class="lineno"><a class="lineno" href="#L155">155</a></span>
<span id="L156" class="lineno"><a class="lineno" href="#L156">156</a></span>
<span id="L157" class="lineno"><a class="lineno" href="#L157">157</a></span>
<span id="L158" class="lineno"><a class="lineno" href="#L158">158</a></span>
<span id="L159" class="lineno"><a class="lineno" href="#L159">159</a></span>
<span id="L160" class="lineno"><a class="lineno" href="#L160">160</a></span>
<span id="L161" class="lineno"><a class="lineno" href="#L161">161</a></span>
<span id="L162" class="lineno"><a class="lineno" href="#L162">162</a></span>
<span id="L163" class="lineno"><a class="lineno" href="#L163">163</a></span>
<span id="L164" class="lineno"><a class="lineno" href="#L164">164</a></span>
<span id="L165" class="lineno"><a class="lineno" href="#L165">165</a></span>
<span id="L166" class="lineno"><a class="lineno" href="#L166">166</a></span>
<span id="L167" class="lineno"><a class="lineno" href="#L167">167</a></span>
<span id="L168" class="lineno"><a class="lineno" href="#L168">168</a></span>
<span id="L169" class="lineno"><a class="lineno" href="#L169">169</a></span>
<span id="L170" class="lineno"><a class="lineno" href="#L170">170</a></span>
<span id="L171" class="lineno"><a class="lineno" href="#L171">171</a></span>
<span id="L172" class="lineno"><a class="lineno" href="#L172">172</a></span>
<span id="L173" class="lineno"><a class="lineno" href="#L173">173</a></span>
<span id="L174" class="lineno"><a class="lineno" href="#L174">174</a></span>
<span id="L175" class="lineno"><a class="lineno" href="#L175">175</a></span>
<span id="L176" class="lineno"><a class="lineno" href="#L176">176</a></span>
<span id="L177" class="lineno"><a class="lineno" href="#L177">177</a></span>
<span id="L178" class="lineno"><a class="lineno" href="#L178">178</a></span>
<span id="L179" class="lineno"><a class="lineno" href="#L179">179</a></span>
<span id="L180" class="lineno"><a class="lineno" href="#L180">180</a></span>
<span id="L181" class="lineno"><a class="lineno" href="#L181">181</a></span>
</pre></td>
<td class="table-code"><pre><span class="line-any" title="No Anys on this line!">import torch</span>
<span class="line-any" title="No Anys on this line!">from torch_geometric.nn import MessagePassing</span>
<span class="line-any" title="No Anys on this line!">import torch.nn.functional as F</span>
<span class="line-any" title="No Anys on this line!">from ogb.graphproppred.mol_encoder import BondEncoder</span>
<span class="line-any" title="No Anys on this line!">from torch_geometric.utils import degree</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-precise" title="No Anys on this line!">from typing import Optional</span>
<span class="line-any" title="No Anys on this line!">from torch_geometric.typing import OptTensor</span>
<span class="line-any" title="No Anys on this line!">from torch.nn import Parameter</span>
<span class="line-any" title="No Anys on this line!">from torch_geometric.nn.inits import zeros</span>
<span class="line-any" title="No Anys on this line!">from torch_geometric.utils import get_laplacian</span>
<span class="line-any" title="No Anys on this line!">from torch_geometric.utils import remove_self_loops, add_self_loops, segregate_self_loops</span>
<span class="line-any" title="No Anys on this line!">from torch_sparse import coalesce</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-precise" title="No Anys on this line!">import pdb</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-empty" title="No Anys on this line!">### GIN convolution along the graph structure</span>
<span class="line-precise" title="No Anys on this line!">class GINConvNew(MessagePassing):</span>
<span class="line-any" title="No Anys on this line!">    def __init__(self, emb_dim, dataset_group):</span>
<span class="line-empty" title="No Anys on this line!">        '''</span>
<span class="line-empty" title="No Anys on this line!">            emb_dim (int): node embedding dimensionality</span>
<span class="line-empty" title="No Anys on this line!">        '''</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)">        super(GINConvNew, self).__init__(aggr = "add")</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x15)
Unimported (x3)">        self.mlp = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2 * emb_dim), torch.nn.BatchNorm1d(2 * emb_dim),</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x9)
Unimported (x2)">                                       torch.nn.ReLU(), torch.nn.Linear(2 * emb_dim, emb_dim))</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x6)
Unimported (x2)">        self.eps = torch.nn.Parameter(torch.Tensor([0]))</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        if dataset_group == 'mol' :</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x3)
Unimported (x1)">            self.edge_encoder = BondEncoder(emb_dim = emb_dim)</span>
<span class="line-empty" title="No Anys on this line!">        else :</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x5)
Unimported (x1)">            self.edge_encoder = torch.nn.Linear(7, emb_dim)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="No Anys on this line!">    def forward(self, x, edge_index, edge_attr):</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x4)">        edge_embedding = self.edge_encoder(edge_attr)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x13)">        out = self.mlp((1 + self.eps) *x + self.propagate(edge_index, x=x, edge_attr=edge_embedding))</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)">        return out</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="No Anys on this line!">    def message(self, x_j, edge_attr):</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x5)
Unimported (x1)">        return F.relu(x_j + edge_attr)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="No Anys on this line!">    def update(self, aggr_out):</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)">        return aggr_out</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-empty" title="No Anys on this line!">### GCN convolution along the graph structure</span>
<span class="line-precise" title="No Anys on this line!">class GCNConvNew(MessagePassing):</span>
<span class="line-any" title="No Anys on this line!">    def __init__(self, emb_dim, dataset_group):</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)">        super(GCNConvNew, self).__init__(aggr='add')</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x6)
Unimported (x1)">        self.linear = torch.nn.Linear(emb_dim, emb_dim)</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x5)
Unimported (x1)">        self.root_emb = torch.nn.Embedding(1, emb_dim)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        if dataset_group == 'mol' :</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x3)
Unimported (x1)">            self.edge_encoder = BondEncoder(emb_dim = emb_dim)</span>
<span class="line-empty" title="No Anys on this line!">        else :</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x5)
Unimported (x1)">            self.edge_encoder = torch.nn.Linear(7, emb_dim)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="No Anys on this line!">    def forward(self, x, edge_index, edge_attr):</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x4)">        x = self.linear(x)</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x4)">        edge_embedding = self.edge_encoder(edge_attr)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x3)">        row, col = edge_index</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-empty" title="No Anys on this line!">        # edge_weight = torch.ones((edge_index.size(1), ), device=edge_index.device)</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x9)
Unimported (x1)">        deg = degree(row, x.size(0), dtype=x.dtype) + 1</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x5)">        deg_inv_sqrt = deg.pow(-0.5)</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x4)">        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x8)">        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x10)
Unimported (x1)">        return self.propagate(edge_index, x=x, edge_attr=edge_embedding, norm=norm) + F.relu(</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x9)">            x + self.root_emb.weight) * 1. / deg.view(-1, 1)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="No Anys on this line!">    def message(self, x_j, edge_attr, norm):</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x10)
Unimported (x1)">        return norm.view(-1, 1) * F.relu(x_j + edge_attr)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="No Anys on this line!">    def update(self, aggr_out):</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)">        return aggr_out</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-precise" title="No Anys on this line!">class ChebConvNew(MessagePassing):</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)">    def __init__(self, emb_dim: int, K: int, dataset_group: str,</span>
<span class="line-precise" title="No Anys on this line!">                 normalization: Optional[str] = 'sym', bias: bool = True,</span>
<span class="line-empty" title="No Anys on this line!">                 **kwargs):</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x4)">        kwargs.setdefault('aggr', 'add')</span>
<span class="line-imprecise" title="Any Types on this line: 
Unannotated (x2)">        super(ChebConvNew, self).__init__(**kwargs)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-precise" title="No Anys on this line!">        assert K &gt; 0</span>
<span class="line-precise" title="No Anys on this line!">        assert normalization in [None, 'sym', 'rw'], 'Invalid normalization'</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-precise" title="No Anys on this line!">        if dataset_group == 'mol' :</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x3)">            self.edge_encoder = BondEncoder(emb_dim = emb_dim)</span>
<span class="line-empty" title="No Anys on this line!">        else :</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x5)">            self.edge_encoder = torch.nn.Linear(7, emb_dim)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-precise" title="No Anys on this line!">        self.emb_dim = emb_dim</span>
<span class="line-precise" title="No Anys on this line!">        self.normalization = normalization</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x5)">        self.lins = torch.nn.ModuleList([</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x4)">            torch.nn.Linear(emb_dim, emb_dim, bias=False) for _ in range(K)</span>
<span class="line-empty" title="No Anys on this line!">        ])</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-precise" title="No Anys on this line!">        if bias:</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x6)">            self.bias = Parameter(torch.Tensor(emb_dim))</span>
<span class="line-empty" title="No Anys on this line!">        else:</span>
<span class="line-precise" title="No Anys on this line!">            self.register_parameter('bias', None)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x3)
Unimported (x2)">    def __norm__(self, edge_index, num_nodes: Optional[int],</span>
<span class="line-empty" title="No Anys on this line!">                 edge_weight: OptTensor, normalization: Optional[str],</span>
<span class="line-precise" title="No Anys on this line!">                 lambda_max, dtype: Optional[int] = None,</span>
<span class="line-precise" title="No Anys on this line!">                 batch: OptTensor = None):</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unimported (x4)
Unannotated (x2)">        edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x4)
Unannotated (x2)">        edge_index, edge_weight = get_laplacian(edge_index, edge_weight,</span>
<span class="line-precise" title="No Anys on this line!">                                                normalization, dtype,</span>
<span class="line-precise" title="No Anys on this line!">                                                num_nodes)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x5)
Unimported (x1)">        if batch is not None and lambda_max.numel() &gt; 1:</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x5)
Unimported (x2)">            lambda_max = lambda_max[batch[edge_index[0]]]</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unimported (x4)
Unannotated (x1)">        edge_weight = (2.0 * edge_weight) / lambda_max</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x5)">        edge_weight.masked_fill_(edge_weight == float('inf'), 0)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unimported (x4)
Unannotated (x2)">        edge_index, edge_weight = add_self_loops(edge_index, edge_weight,</span>
<span class="line-precise" title="No Anys on this line!">                                                 fill_value=-1.,</span>
<span class="line-precise" title="No Anys on this line!">                                                 num_nodes=num_nodes)</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x1)">        assert edge_weight is not None</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)
Unimported (x1)">        return edge_index, edge_weight</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x3)
Unimported (x3)">    def forward(self, x, edge_index, edge_attr: OptTensor = None,</span>
<span class="line-precise" title="No Anys on this line!">                batch: OptTensor = None, lambda_max: OptTensor = None):</span>
<span class="line-empty" title="No Anys on this line!">        """"""</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x4)">        edge_embedding = self.edge_encoder(edge_attr)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-imprecise" title="Any Types on this line: 
Unimported (x1)">        if self.normalization != 'sym' and lambda_max is None:</span>
<span class="line-precise" title="No Anys on this line!">            raise ValueError('You need to pass `lambda_max` to `forward() in`'</span>
<span class="line-empty" title="No Anys on this line!">                             'case the normalization is non-symmetric.')</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-imprecise" title="Any Types on this line: 
Unimported (x1)">        if lambda_max is None:</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x4)
Unannotated (x4)">            lambda_max = torch.tensor(2.0, dtype=x.dtype, device=x.device)</span>
<span class="line-any" title="Any Types on this line: 
Explicit (x1)
Unimported (x3)">        if not isinstance(lambda_max, torch.Tensor):</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x5)
Unannotated (x2)">            lambda_max = torch.tensor(lambda_max, dtype=x.dtype,</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">                                      device=x.device)</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x1)">        assert lambda_max is not None</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x10)
Unimported (x2)">        edge_index, norm = self.__norm__(edge_index, x.size(self.node_dim),</span>
<span class="line-precise" title="No Anys on this line!">                                         None, self.normalization,</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x1)
Unannotated (x2)">                                         lambda_max, dtype=x.dtype,</span>
<span class="line-imprecise" title="Any Types on this line: 
Unimported (x1)">                                         batch=batch)</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x2)
Unannotated (x10)">        edge_index, norm = coalesce(edge_index, norm, m=x.shape[0], n=x.shape[0])</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x4)
Unannotated (x4)">        edge_index, norm, loop_edge_index, loop_norm = segregate_self_loops(edge_index, norm)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        Tx_0 = x</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        Tx_1 = x  # Dummy.</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x4)
Unannotated (x1)">        out = self.lins[0](Tx_0)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-empty" title="No Anys on this line!">        # propagate_type: (x: Tensor, norm: Tensor)</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x1)">        if len(self.lins) &gt; 1:</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x4)
Unimported (x1)">            Tx_1 = self.propagate(edge_index, x=x, edge_attr=edge_embedding, norm=norm, size=None)+\</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x7)
Unannotated (x1)">                   loop_norm.view(-1,1)*F.relu(x)</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x6)
Unannotated (x1)">            out = out + self.lins[1](Tx_1)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unimported (x3)">        for lin in self.lins[2:]:</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x3)
Unimported (x1)">            Tx_2 = self.propagate(edge_index, x=Tx_1, edge_attr=edge_embedding, norm=norm, size=None)+\</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x7)
Unannotated (x1)">                   loop_norm.view(-1,1)*F.relu(Tx_1)</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)">            Tx_2 = 2. * Tx_2 - Tx_0</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x6)">            out = out + lin.forward(Tx_2)</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x3)">            Tx_0, Tx_1 = Tx_1, Tx_2</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unimported (x1)">        if self.bias is not None:</span>
<span class="line-any" title="Any Types on this line: 
Unimported (x2)">            out += self.bias</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unimported (x1)">        return out</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="No Anys on this line!">    def message(self, x_j, edge_attr, norm):</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x10)
Unimported (x1)">        return norm.view(-1, 1) * F.relu(x_j+edge_attr)</span>
</pre></td>
</tr></tbody>
</table>
</body>
</html>
