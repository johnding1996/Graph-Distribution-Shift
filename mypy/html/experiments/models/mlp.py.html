<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" type="text/css" href="../../../mypy-html.css">
</head>
<body>
<h2>experiments.models.mlp</h2>
<table>
<caption>experiments/models/mlp.py</caption>
<tbody><tr>
<td class="table-lines"><pre><span id="L1" class="lineno"><a class="lineno" href="#L1">1</a></span>
<span id="L2" class="lineno"><a class="lineno" href="#L2">2</a></span>
<span id="L3" class="lineno"><a class="lineno" href="#L3">3</a></span>
<span id="L4" class="lineno"><a class="lineno" href="#L4">4</a></span>
<span id="L5" class="lineno"><a class="lineno" href="#L5">5</a></span>
<span id="L6" class="lineno"><a class="lineno" href="#L6">6</a></span>
<span id="L7" class="lineno"><a class="lineno" href="#L7">7</a></span>
<span id="L8" class="lineno"><a class="lineno" href="#L8">8</a></span>
<span id="L9" class="lineno"><a class="lineno" href="#L9">9</a></span>
<span id="L10" class="lineno"><a class="lineno" href="#L10">10</a></span>
<span id="L11" class="lineno"><a class="lineno" href="#L11">11</a></span>
<span id="L12" class="lineno"><a class="lineno" href="#L12">12</a></span>
<span id="L13" class="lineno"><a class="lineno" href="#L13">13</a></span>
<span id="L14" class="lineno"><a class="lineno" href="#L14">14</a></span>
<span id="L15" class="lineno"><a class="lineno" href="#L15">15</a></span>
<span id="L16" class="lineno"><a class="lineno" href="#L16">16</a></span>
<span id="L17" class="lineno"><a class="lineno" href="#L17">17</a></span>
<span id="L18" class="lineno"><a class="lineno" href="#L18">18</a></span>
<span id="L19" class="lineno"><a class="lineno" href="#L19">19</a></span>
<span id="L20" class="lineno"><a class="lineno" href="#L20">20</a></span>
<span id="L21" class="lineno"><a class="lineno" href="#L21">21</a></span>
<span id="L22" class="lineno"><a class="lineno" href="#L22">22</a></span>
<span id="L23" class="lineno"><a class="lineno" href="#L23">23</a></span>
<span id="L24" class="lineno"><a class="lineno" href="#L24">24</a></span>
<span id="L25" class="lineno"><a class="lineno" href="#L25">25</a></span>
<span id="L26" class="lineno"><a class="lineno" href="#L26">26</a></span>
<span id="L27" class="lineno"><a class="lineno" href="#L27">27</a></span>
<span id="L28" class="lineno"><a class="lineno" href="#L28">28</a></span>
<span id="L29" class="lineno"><a class="lineno" href="#L29">29</a></span>
<span id="L30" class="lineno"><a class="lineno" href="#L30">30</a></span>
<span id="L31" class="lineno"><a class="lineno" href="#L31">31</a></span>
<span id="L32" class="lineno"><a class="lineno" href="#L32">32</a></span>
<span id="L33" class="lineno"><a class="lineno" href="#L33">33</a></span>
<span id="L34" class="lineno"><a class="lineno" href="#L34">34</a></span>
<span id="L35" class="lineno"><a class="lineno" href="#L35">35</a></span>
<span id="L36" class="lineno"><a class="lineno" href="#L36">36</a></span>
<span id="L37" class="lineno"><a class="lineno" href="#L37">37</a></span>
<span id="L38" class="lineno"><a class="lineno" href="#L38">38</a></span>
<span id="L39" class="lineno"><a class="lineno" href="#L39">39</a></span>
<span id="L40" class="lineno"><a class="lineno" href="#L40">40</a></span>
<span id="L41" class="lineno"><a class="lineno" href="#L41">41</a></span>
<span id="L42" class="lineno"><a class="lineno" href="#L42">42</a></span>
<span id="L43" class="lineno"><a class="lineno" href="#L43">43</a></span>
<span id="L44" class="lineno"><a class="lineno" href="#L44">44</a></span>
<span id="L45" class="lineno"><a class="lineno" href="#L45">45</a></span>
<span id="L46" class="lineno"><a class="lineno" href="#L46">46</a></span>
<span id="L47" class="lineno"><a class="lineno" href="#L47">47</a></span>
<span id="L48" class="lineno"><a class="lineno" href="#L48">48</a></span>
<span id="L49" class="lineno"><a class="lineno" href="#L49">49</a></span>
<span id="L50" class="lineno"><a class="lineno" href="#L50">50</a></span>
<span id="L51" class="lineno"><a class="lineno" href="#L51">51</a></span>
<span id="L52" class="lineno"><a class="lineno" href="#L52">52</a></span>
<span id="L53" class="lineno"><a class="lineno" href="#L53">53</a></span>
<span id="L54" class="lineno"><a class="lineno" href="#L54">54</a></span>
<span id="L55" class="lineno"><a class="lineno" href="#L55">55</a></span>
<span id="L56" class="lineno"><a class="lineno" href="#L56">56</a></span>
<span id="L57" class="lineno"><a class="lineno" href="#L57">57</a></span>
<span id="L58" class="lineno"><a class="lineno" href="#L58">58</a></span>
<span id="L59" class="lineno"><a class="lineno" href="#L59">59</a></span>
<span id="L60" class="lineno"><a class="lineno" href="#L60">60</a></span>
<span id="L61" class="lineno"><a class="lineno" href="#L61">61</a></span>
<span id="L62" class="lineno"><a class="lineno" href="#L62">62</a></span>
<span id="L63" class="lineno"><a class="lineno" href="#L63">63</a></span>
<span id="L64" class="lineno"><a class="lineno" href="#L64">64</a></span>
<span id="L65" class="lineno"><a class="lineno" href="#L65">65</a></span>
<span id="L66" class="lineno"><a class="lineno" href="#L66">66</a></span>
<span id="L67" class="lineno"><a class="lineno" href="#L67">67</a></span>
<span id="L68" class="lineno"><a class="lineno" href="#L68">68</a></span>
<span id="L69" class="lineno"><a class="lineno" href="#L69">69</a></span>
<span id="L70" class="lineno"><a class="lineno" href="#L70">70</a></span>
<span id="L71" class="lineno"><a class="lineno" href="#L71">71</a></span>
<span id="L72" class="lineno"><a class="lineno" href="#L72">72</a></span>
<span id="L73" class="lineno"><a class="lineno" href="#L73">73</a></span>
<span id="L74" class="lineno"><a class="lineno" href="#L74">74</a></span>
<span id="L75" class="lineno"><a class="lineno" href="#L75">75</a></span>
<span id="L76" class="lineno"><a class="lineno" href="#L76">76</a></span>
<span id="L77" class="lineno"><a class="lineno" href="#L77">77</a></span>
<span id="L78" class="lineno"><a class="lineno" href="#L78">78</a></span>
<span id="L79" class="lineno"><a class="lineno" href="#L79">79</a></span>
<span id="L80" class="lineno"><a class="lineno" href="#L80">80</a></span>
<span id="L81" class="lineno"><a class="lineno" href="#L81">81</a></span>
<span id="L82" class="lineno"><a class="lineno" href="#L82">82</a></span>
<span id="L83" class="lineno"><a class="lineno" href="#L83">83</a></span>
<span id="L84" class="lineno"><a class="lineno" href="#L84">84</a></span>
<span id="L85" class="lineno"><a class="lineno" href="#L85">85</a></span>
<span id="L86" class="lineno"><a class="lineno" href="#L86">86</a></span>
<span id="L87" class="lineno"><a class="lineno" href="#L87">87</a></span>
<span id="L88" class="lineno"><a class="lineno" href="#L88">88</a></span>
<span id="L89" class="lineno"><a class="lineno" href="#L89">89</a></span>
</pre></td>
<td class="table-code"><pre><span class="line-any" title="No Anys on this line!">import torch</span>
<span class="line-any" title="No Anys on this line!">import torch.nn.functional as F</span>
<span class="line-any" title="No Anys on this line!">from ogb.graphproppred.mol_encoder import AtomEncoder</span>
<span class="line-any" title="No Anys on this line!">from torch_geometric.nn import global_mean_pool</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-precise" title="No Anys on this line!">class MLP(torch.nn.Module):</span>
<span class="line-any" title="No Anys on this line!">    def __init__(self, gnn_type, dataset_group, num_tasks=128, num_layers=5, emb_dim=300, dropout=0.5, is_pooled=True,</span>
<span class="line-empty" title="No Anys on this line!">                 **model_kwargs):</span>
<span class="line-empty" title="No Anys on this line!">        """</span>
<span class="line-empty" title="No Anys on this line!">        Args:</span>
<span class="line-empty" title="No Anys on this line!">            - num_tasks (int): number of binary label tasks. default to 128 (number of tasks of ogbg-molpcba)</span>
<span class="line-empty" title="No Anys on this line!">            - num_layers (int): number of message passing layers of GNN</span>
<span class="line-empty" title="No Anys on this line!">            - emb_dim (int): dimensionality of hidden channels</span>
<span class="line-empty" title="No Anys on this line!">            - dropout (float): dropout ratio applied to hidden channels</span>
<span class="line-empty" title="No Anys on this line!">        """</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        self.dataset_group = dataset_group</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)">        super(MLP, self).__init__()</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        self.num_layers = num_layers</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        self.dropout = dropout</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        self.emb_dim = emb_dim</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        self.num_tasks = num_tasks</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        self.is_pooled = is_pooled</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)">        if num_tasks is None:</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">            self.d_out = self.emb_dim</span>
<span class="line-empty" title="No Anys on this line!">        else:</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">            self.d_out = self.num_tasks</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        if self.num_layers &lt; 2:</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)">            raise ValueError("Number of GNN layers must be greater than 1.")</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-empty" title="No Anys on this line!">        ##################################################################################</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        if self.dataset_group == 'mol':</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x3)
Unimported (x1)">            self.node_encoder = AtomEncoder(emb_dim)</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        elif self.dataset_group == 'ppa':</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x5)
Unimported (x1)">            self.node_encoder = torch.nn.Embedding(1, emb_dim) # uniform input node embedding</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        elif self.dataset_group == 'RotatedMNIST':</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x5)
Unimported (x1)">            self.node_encoder = torch.nn.Linear(1, emb_dim)</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        elif self.dataset_group == 'ColoredMNIST' :</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x5)
Unimported (x1)">            self.node_encoder = torch.nn.Linear(2, emb_dim)</span>
<span class="line-empty" title="No Anys on this line!">            # self.node_encoder_cate = torch.nn.Embedding(8, emb_dim)</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        elif self.dataset_group == 'SBM' :</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x5)
Unimported (x1)">            self.node_encoder = torch.nn.Embedding(8, emb_dim)</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x2)">        elif self.dataset_group == 'UPFD':</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x5)
Unimported (x1)">            self.node_encoder = torch.nn.Embedding(8, emb_dim)</span>
<span class="line-empty" title="No Anys on this line!">        else:</span>
<span class="line-precise" title="No Anys on this line!">            raise NotImplementedError</span>
<span class="line-empty" title="No Anys on this line!">        ###List of GNNs</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x4)
Unimported (x1)">        self.fcs = torch.nn.ModuleList()</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x4)
Unimported (x1)">        self.batch_norms = torch.nn.ModuleList()</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x3)">        for layer in range(num_layers):</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x8)
Unimported (x1)">            self.fcs.append(torch.nn.Linear(emb_dim, emb_dim))</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x7)
Unimported (x1)">            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))</span>
<span class="line-empty" title="No Anys on this line!">        ##################################################################################</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unimported (x1)
Unannotated (x1)">        self.pool = global_mean_pool</span>
<span class="line-empty" title="No Anys on this line!">        # Pooling function to generate whole-graph embeddings</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)">        if num_tasks is None:</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)">            self.graph_pred_linear = None</span>
<span class="line-empty" title="No Anys on this line!">        else:</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x6)
Unimported (x1)">            self.graph_pred_linear = torch.nn.Linear(self.emb_dim, self.num_tasks)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="No Anys on this line!">    def forward(self, batched_data):</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x3)">        x = batched_data.x</span>
<span class="line-empty" title="No Anys on this line!">        # if self.dataset_group == 'ColoredMNIST' :</span>
<span class="line-empty" title="No Anys on this line!">        #     # x = self.node_encoder(x[:, :2]) + self.node_encoder_cate(x[:, 2:].to(torch.int).squeeze())</span>
<span class="line-empty" title="No Anys on this line!">        #     x = self.node_encoder(x[:, :2])</span>
<span class="line-empty" title="No Anys on this line!">        # else :</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x4)">        x = self.node_encoder(x)</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x3)">        for i in range(self.num_layers) :</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x6)">            x = self.fcs[i](x)</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x6)">            x = self.batch_norms[i](x)</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x4)">            if i == self.num_layers - 1:</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x6)
Unimported (x1)">                x = F.dropout(x, self.dropout, training=self.training)</span>
<span class="line-empty" title="No Anys on this line!">            else:</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x8)
Unimported (x2)">                x = F.dropout(F.relu(x), self.dropout, training=self.training)</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x6)">        x = self.pool(x, batched_data.batch)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)">        if self.graph_pred_linear is None:</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x1)">            return x</span>
<span class="line-empty" title="No Anys on this line!">        else:</span>
<span class="line-any" title="Any Types on this line: 
Unannotated (x3)">            return self.graph_pred_linear(x)</span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-empty" title="No Anys on this line!"></span>
<span class="line-empty" title="No Anys on this line!"></span>
</pre></td>
</tr></tbody>
</table>
</body>
</html>
